from typing import Optional
from pydantic import Field

from agieval.core.plugin.plugins_decorator import InferAgentPlugin
from agieval.entity.request_state import RequestState
from agieval.entity.plugin_param.step_param import BaseAgentPluginParam
from agieval.plugin.agent.base_agent import BaseAgent
from agieval.plugin.model.base_model import BaseModel


class ModelScoreZeroshotV3AgentParam(BaseAgentPluginParam):
    """Model scoring Agent parameters"""
    multiple_choice_type: Optional[str] = Field(default=None,
                                                description="Multiple choice question scoring type: all-requires complete consistency, percentage-partially correct gets 0.5 points, others-correct for one is enough")
    score_max_prompt_length: int = Field(default=4500, description="Maximum prompt length, will be truncated if exceeded")
    score_max_new_tokens: int = Field(default=20, description="Maximum number of tokens generated by the scoring model")


@InferAgentPlugin
class ModelScoreZeroshotV3Agent(BaseAgent[ModelScoreZeroshotV3AgentParam]):
    """
    Model scoring Agent - used to automatically score model inference results

    This Agent will:
    1. Extract the model's answer
    2. Construct scoring prompt (including question, correct answer, model response)
    3. Call scoring model for scoring
    4. Store scoring result in request_state.model_score_result
    """

    def run_one(self, model: BaseModel, request_state: RequestState) -> RequestState:
        """
        Score a single request state with the model

        Args:
            model: Scoring model
            request_state: Request state containing original question and model response

        Returns:
            request_state with updated model_score_result
        """
        # Check if there is already an inference result
        if request_state.result is None or not request_state.result.completions:
            self.log_warn(f"Instance {request_state.instance.id} has no inference result, skipping scoring")
            return request_state

        # Construct scoring prompt
        try:
            score_prompt = self._build_score_prompt(request_state)

            # Construct scoring request
            from agieval.entity.request import Request, Message

            score_request = Request(
                messages=[
                    Message(role="user", content=score_prompt)
                ],
                max_new_tokens=self.context_param.score_max_new_tokens,
                temperature=0.0,  # Scoring uses fixed parameters
            )

            # Call scoring model
            score_result = model.run(score_request)

            # Save scoring result
            if score_result and score_result.completions:
                request_state.model_score_result = score_result.completions[0].text
                request_state.model_score_request = score_prompt
                self.log(f"Instance {request_state.instance.id} scoring completed: {request_state.model_score_result}")
            else:
                self.log_error(f"Instance {request_state.instance.id} scoring failed: Scoring model returned no result")
                request_state.model_score_result = None

        except Exception as e:
            self.log_error(f"Instance {request_state.instance.id} scoring exception: {e}")
            request_state.model_score_result = None

        return request_state

    def _build_score_prompt(self, request_state: RequestState) -> str:
        """
        Construct the actual request prompt for the scoring model

        Args:
            request_state: Request state

        Returns:
            Complete scoring prompt
        """
        # 1. Get original question
        origin_prompt = request_state.instance.input.text

        # 2. If there is an option mapping (multiple choice), concatenate options
        if request_state.output_mapping is not None:
            option_text = ""
            for key, value in request_state.output_mapping.items():
                option_text += f"{key}. {value}\n"
            origin_prompt = origin_prompt + "\n" + option_text

        # 3. Extract model response
        origin_completion = request_state.result.completions[0].text
        # Clean special markers
        if origin_completion:
            origin_completion = origin_completion.replace("<|endoftext|>", "\n")
        else:
            origin_completion = ''

        # Extract answer (if there is custom extraction logic, it can be overridden in subclasses)
        extracted_completion = self._extract_answer(origin_completion)
        request_state.extra_data = {
            "extracted_completion_for_score": extracted_completion
        }

        # 4. Construct correct answer
        correct_answer = self._build_correct_answer(request_state)

        # 5. Truncate overly long prompts (to avoid exceeding scoring model context length)
        if len(origin_prompt) > self.context_param.score_max_prompt_length:
            half_length = self.context_param.score_max_prompt_length // 2
            origin_prompt = origin_prompt[:half_length] + origin_prompt[-half_length:]
            self.log_debug(
                f"Instance {request_state.instance.id} prompt is too long, truncated to {self.context_param.score_max_prompt_length} characters")

        # 6. Construct complete scoring prompt
        final_prompt = self._build_eval_prompt(origin_prompt, correct_answer, extracted_completion, request_state)

        return final_prompt

    def _extract_answer(self, completion: str) -> str:
        """
        Extract answer from model output

        Default implementation returns the original text directly, subclasses can override to implement custom extraction logic

        Args:
            completion: Model's original output

        Returns:
            Extracted answer
        """
        return completion

    def _build_correct_answer(self, request_state: RequestState) -> str:
        """
        Construct correct answer text

        Args:
            request_state: Request state

        Returns:
            Text description of the correct answer
        """
        correct_texts = []

        # Collect all reference answers marked as correct
        for reference in request_state.instance.references:
            if reference.is_correct:
                correct_text = reference.output.text
                correct_texts.append(correct_text)

        # If there is an output mapping (multiple choice), add option identifiers
        if request_state.output_mapping:
            for i in range(len(correct_texts)):
                for key, value in request_state.output_mapping.items():
                    if value == correct_texts[i]:
                        correct_texts[i] = f"{key}. {correct_texts[i]}"
                        break

        # Construct different scoring descriptions based on the number of answers
        if len(correct_texts) > 1:
            # Multiple choice question
            multiple_choice_type = self.context_param.multiple_choice_type

            if multiple_choice_type == "all":
                # Require complete consistency
                correct_answer = "需要回答出以下答案并没有其他多余选项：" + ",".join(correct_texts)
            elif multiple_choice_type == "percentage":
                # Partially correct gets partial score
                correct_answer = (
                        "这是一个多选题，当学生回答包含错误选项时给0分；"
                        "当学生回答上部分正确选项但没有包含全部正确选项给0.5分；"
                        "当模型回答了全部正确选项且不包含错误选项给1分。"
                        "以下是所有正确选项: " + ",".join(correct_texts)
                )
            else:
                # Correct for any one is enough
                correct_answer = "这是一个多选题，当学生回答在下面列出的答案中时为正确，否则为错误: " + ",".join(
                    correct_texts)
        else:
            # Single choice or QA question
            correct_answer = correct_texts[0] if correct_texts else ""

        return correct_answer

    def _build_eval_prompt(self, origin_prompt: str, correct_answer: str,
                           completion: str, request_state: RequestState) -> str:
        """
        Construct complete scoring prompt

        Args:
            origin_prompt: Original question
            correct_answer: Correct answer
            completion: Model response
            request_state: Request state (for logging)

        Returns:
            Complete scoring prompt
        """
        # Clean special markers in prompt
        if origin_prompt is not None:
            origin_prompt = origin_prompt.replace("ASSISTANT: ASSISTANT: ", "ASSISTANT: ")
            origin_prompt = origin_prompt.replace('ASSISTANT: "; "参考正确答案', '"; "参考正确答案')
            origin_prompt = origin_prompt.replace("ASSISTANT:", " ")
            origin_prompt = origin_prompt.replace("ASSISTANT：", " ")
        else:
            self.log_warn(f"Instance {request_state.instance.id} missing origin_prompt")
            origin_prompt = ""

        if correct_answer is None:
            self.log_warn(f"Instance {request_state.instance.id} missing correct_answer")
            correct_answer = ""

        # Construct scoring prompt template
        result = (
            '现在开始评判: 学生考卷：{'
            '"问题": "###QUESTION###"; '
            '"参考正确答案": "###CORRECT_ANSWER###";'
            '"学生回答": "###model_answer###"'
            '}，请输出评分:\nASSISTANT:'
        )

        # Replace placeholders
        result = result.replace("###QUESTION###", origin_prompt)
        result = result.replace("###CORRECT_ANSWER###", correct_answer)
        result = result.replace("###model_answer###", completion)

        # Add system instructions
        system_cmd = self._build_eval_system_cmd()
        result = system_cmd + "USER: " + result

        return result

    def _build_eval_system_cmd(self) -> str:
        """
        Construct scoring system instructions

        Returns:
            System instruction text
        """
        instruction_string = (
            "SYSTEM: 你是一个阅卷评分老师。"
            "给你一个问题, 请参考正确答案，判断学生回答是否正确。"
            "答案正确请给1分，答案错误给0分。"
            '采用json格式返回，返回结构示例：{"老师评分": 0/1}\n'
        )
        return instruction_string