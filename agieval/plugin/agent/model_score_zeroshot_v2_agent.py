import traceback
from typing import Optional
from pydantic import Field
from agieval.core.plugin.plugins_decorator import InferAgentPlugin
from agieval.entity.request_state import RequestState
from agieval.entity.plugin_param.step_param import BaseAgentPluginParam
from agieval.plugin.agent.base_agent import BaseAgent
from agieval.plugin.model.base_model import BaseModel
from agieval.common.logger import log


class ModelScoreZeroshotV2AgentParam(BaseAgentPluginParam):
    """Model scoring Agent parameters"""
    multiple_choice_type: Optional[str] = Field(default=None, description="Multiple choice question scoring type: all-requires complete consistency, percentage-partially correct gets 0.5 points, others-correct for one is enough")
    score_max_prompt_length: int = Field(default=4500, description="Maximum prompt length, will be truncated if exceeded")
    score_max_new_tokens: int = Field(default=20, description="Maximum number of tokens generated by the scoring model")
    model_score_extract_prefix: Optional[str] = Field(default=None, description="Scoring prompt prefix")
    model_score_extract_suffix: Optional[str] = Field(default=None, description="Scoring prompt suffix")


@InferAgentPlugin
class ModelScoreZeroshotV2Agent(BaseAgent[ModelScoreZeroshotV2AgentParam]):
    """

    """

    def run_one(self, model: BaseModel, request_state: RequestState) -> RequestState:
        """
        AgentWrapper entry method. Receives all instance tasks issued by a job, then executes.
        :param job:
        :return:
        """
        # Process scoring prompt

        try:
            # Construct scoring request
            self.preprocess_score_data(request_state)
            from agieval.entity.request import Request, Message

            score_request = Request(
                messages=[
                    Message(role="user", content=request_state.model_score_request)
                ],
                max_new_tokens=self.context_param.score_max_new_tokens,
                temperature=0.0,  # Scoring uses fixed parameters
            )

            # Call scoring model
            score_result = model.run(score_request)
            print(f"score_result: {score_result}")

            # Save scoring result
            if score_result and score_result.completions:
                request_state.model_score_result = score_result.completions[0].text
                log(f"Instance {request_state.instance.id} scoring completed: {request_state.model_score_result}")
            else:
                log(f"Instance {request_state.instance.id} scoring failed: Scoring model returned no result")
                request_state.model_score_result = None

        except Exception as e:
            traceback.print_exc()
            log(f"Instance {request_state.instance.id} scoring exception: {e}")
            request_state.model_score_result = None

        return request_state

    def preprocess_score_data(self, request_state: RequestState):
        """
        Construct actual request data for the scoring model
        """
        origin_prompt = request_state.instance.input.text
        origin_completion = request_state.result.completions[0].text
        origin_completion = origin_completion.replace("<|endoftext|>", "\n") if origin_completion is not None else ''
        origin_completion = self.extract_answer_for_model_eval(origin_completion)
        request_state.extra_data = {"extracted_completion_for_score": origin_completion}
        correct_answer = self.build_correct_result(request_state)
        # Truncate origin_prompt text length to 2200, truncate from the middle, calculation logic, maximum length 4096 tokens, system prompt takes about 500 tokens, remaining available length about 3500/1.6
        # Appropriately increase text length to 8K
        if len(origin_prompt) > self.context_param.score_max_prompt_length:
            half_length = self.context_param.score_max_prompt_length // 2
            origin_prompt = origin_prompt[:half_length] + origin_prompt[-half_length:]
        prompt = self.build_eval_prompt(origin_prompt, correct_answer, origin_completion, request_state)
        request_state.model_score_request = prompt

    def build_correct_result(self, request_state: RequestState):
        correct_texts = []

        for reference in request_state.instance.references:
            if reference.is_correct:
                correct_text = reference.output.text
                correct_texts.append(correct_text)
        correct_keys = []
        if request_state.output_mapping:
            for key, value in request_state.output_mapping.items():
                for i in range(len(correct_texts)):
                    if value == correct_texts[i]:
                        correct_keys.append(key)
                        correct_texts[i] = key + ". " + correct_texts[i]
                        break

        if len(correct_texts) > 1:
            # Require model and answer to be completely consistent
            if self.context_param.multiple_choice_type == "all":
                correct_answer = "需要回答出以下答案并没有其他多余选项：" + ",".join(correct_texts)
            # Model completely correct gets 1 point, partially correct without wrong options gets 0.5 points, otherwise gets 0 points
            elif self.context_param.multiple_choice_type  == "percentage":
                correct_answer = "这是一个多选题，当学生回答包含错误选项时给0分；当学生回答上部分正确选项但没有包含全部正确选项给0.5分； 当模型回答了全部正确选项且不包含错误选项给1分。以下是所有正确选项: " + ",".join(
                    correct_texts)
            # Model just needs to answer one correctly to get 1 point
            else:
                correct_answer = "这是一个多选题，当学生回答在下面列出的答案中时为正确，否则为错误: " + ",".join(
                    correct_texts)
        else:
            correct_answer = correct_texts[0]
        return correct_answer

    def build_eval_prompt(self, origin_prompt: str, correct_answer, completion: str, request_state) -> str:
        result = "现在开始评判: 学生考卷：{\"问题\": \"###QUESTION###\"; \"参考正确答案\": \"###CORRECT_ANSWER###\";\"学生回答\": \"###model_answer###\"}，请输出评分:\nASSISTANT:"
        if origin_prompt is not None:
            origin_prompt = origin_prompt.replace("ASSISTANT: ASSISTANT: ", "ASSISTANT: ").replace(
                'ASSISTANT: "; "参考正确答案', '"; "参考正确答案').replace("ASSISTANT:", " ").replace("ASSISTANT：", " ")
        else:
            self.log("Missing origin_prompt", instance_id=request_state.instance.id)
        result = result.replace("###QUESTION###", origin_prompt)
        if correct_answer is None:
            self.log("Missing correct answer", instance_id=request_state.instance.id)
        result = result.replace("###CORRECT_ANSWER###", correct_answer)
        result = result.replace("###model_answer###", completion)
        result = self.build_eval_system_cmd() + "USER: " + result
        return result

    def build_eval_system_cmd(self) -> str:
        instruction_string = "SYSTEM: 你是一个阅卷评分老师。给你一个问题, 请参考正确答案，判断学生回答是否正确。答案正确请给1分，答案错误给0分，注意如果学生回答和正确答案使用不同的语言，评分为0，评分理由为’语言不一致‘。采用json格式返回，返回结构示例：{\"老师评分\": 0/1}\n"
        return instruction_string

    def extract_answer_for_model_eval(self, text):
        # When scoring models, extract results from the model's original output
        # Temporary hardcoded, later changed to plugin configuration for whether to enable truncation.
        # log(f"self.model_score_extract_prefix: {self.model_score_extract_prefix}")
         #log(f"self.model_score_extract_suffix: {self.model_score_extract_suffix}")
        # log(f"Original text: {text}")
        if self.context_param.model_score_extract_prefix:
            text = text.split(self.context_param.model_score_extract_prefix)
            if len(text) > 1:
                # log(f"Found {self.model_score_extract_prefix}")
                text = text[-1]
                # log(f"Text after truncation: {text}")
            else:
                # log(f"Did not find {self.model_score_extract_prefix}")
                text = text[0]
            text = text.strip()
        if self.context_param.model_score_extract_suffix:
            text = text.split(self.context_param.model_score_extract_suffix)[0]
            text = text.strip()
        # if self.model_score_extract_prefix or self.model_score_extract_suffix:
        #     # log(f"Result extraction: {text} ")
        return text